{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_spCXMT3r128"
   },
   "source": [
    "# Choose a base model for your AI Agent using Atla x Langfuse"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "31K7AlAQrA0f"
   },
   "source": [
    "This notebook demonstrates how to evaluate function calling capabilities across different models using **Atla Selene for evaluation** and **Langfuse for experiment observability**. If you'd like a visual walkthrough, check out our [demo video](https://youtu.be/0lenKLgn1p8).\n",
    "\n",
    "We compare the performance of various models (o1-mini, o3-mini, and gpt-4o) on function calling tasks using the [Salesforce ShareGPT dataset](https://huggingface.co/datasets/arcee-ai/agent-data/viewer/default/train?f%5Bdataset%5D%5Bvalue%5D=%27glaive-function-calling-v2-extended%27&sql=SELECT+*%0AFROM+train%0AWHERE+dataset+%3D+%27salesforce_sharegpt%27%0ALIMIT+10%3B&views%5B%5D=train).\n",
    "\n",
    "The notebook uploads the dataset to Langfuse and sets up experiment runs on different models, where these are automatically evaluated by Selene.\n",
    "\n",
    "<br>\n",
    "\n",
    "**Requirements:**\n",
    "\n",
    "- An Atla account - you can sign up for free [here](https://www.atla-ai.com/sign-up)\n",
    "- A Langfuse account - you can sign up for free [here](https://cloud.langfuse.com/auth/sign-up)\n",
    "- An OpenAI API key - you can sign up for free [here](https://platform.openai.com/signup)\n",
    "\n",
    "**Get started**\n",
    "\n",
    "1. Follow the steps below in **Setup Atla on Langfuse**.\n",
    "2. Set your Langfuse + OpenAI API keys.\n",
    "3. Run the rest of the functions to run experiments on performance across the three OpenAI models. Selene will score the outputs against the ground truths from the dataset.\n",
    "4. Easily compare model outputs and assess average scores in your Langfuse Cloud."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fSh3ZbhjlqAA"
   },
   "source": [
    "## Setup Atla on Langfuse"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "x2rVF81u03cg"
   },
   "source": [
    "Navigate to your project on [cloud.langfuse.com](cloud.langfuse.com):\n",
    "\n",
    "<br>\n",
    "\n",
    "**Add your Atla API key to your Langfuse project:**\n",
    "\n",
    "1. Head to **Settings** → **LLM Connections** and select **+** **Add new LLM API key.**\n",
    "2. Set `atla` as the **Provider name** and select `atla` from the **LLM adapter** dropdown .\n",
    "3. The API Base URL will automatically be filled in. Paste your Atla API key beginning with “pk-…” into the **API Key** field.\n",
    "4. Leave **Enable default models** on.\n",
    "5. Click **Save new LLM API key.**\n",
    "\n",
    "![alt text](https://atla-ai.notion.site/image/attachment%3Aed7c7d92-b0bf-464f-9f43-83df6de65a5e%3Aimage.png?table=block&id=1bc309d1-7745-80fd-9f2d-cdd40f3005cf&spaceId=f08e6e70-73af-4363-9621-90e906b92ebc&width=2000&userId=&cache=v2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MR1Rms-uE1jg"
   },
   "source": [
    "**Add an LLM-as-a-Judge template**\n",
    "\n",
    "1. Head to **Evaluation → LLM-as-a-Judge** in your sidebar and select **Templates**\n",
    "2. Click **+ New Template**\n",
    "3. Select atla as the Model Provider and choose atla-selene as the Model name\n",
    "4. Let’s evaluate the function calling ability of a model - name your template `function_calling` and paste in the following prompt:\n",
    "\n",
    "```\n",
    "Evaluate whether the model accurately selects the appropriate function(s) from the available options with a binary score of 0 or 1, comparing against the provided ground truth.\n",
    "\n",
    "Scoring Criteria:\n",
    "1: The model selection matches the ground truth—selected the correct function(s) using the proper tool call syntax.\n",
    "0: The model selection does not match the ground truth—selected incorrect functions, missed required functions, or used significantly different parameters.\n",
    "\n",
    "Instruction: {{input}}\n",
    "Ground Truth: {{expected_output}}\n",
    "Response: {{output}}\n",
    "```\n",
    "\n",
    "5. Adjust the prompt under **Reasoning** to tune Selene’s evaluation critique to your liking - we use \"One sentence reasoning for the score\"\n",
    "\n",
    "6. Adjust the prompt under **Score** to \"Score 0 or 1\" in alignment with our eval prompt and hit **Save**\n",
    "\n",
    "![alt text](https://atla-ai.notion.site/image/attachment%3A0078240d-4138-4b4c-a00e-075304065051%3Aimage.png?table=block&id=1ba309d1-7745-80fa-bc58-d43390728b0c&spaceId=f08e6e70-73af-4363-9621-90e906b92ebc&width=2000&userId=&cache=v2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OSyZivbsE4LR"
   },
   "source": [
    "**Add a new Evaluator configuration:**\n",
    "\n",
    "1. Head to **Evaluation → LLM-as-a-Judge** in your sidebar and select **Evaluators**\n",
    "2. Click **+ New evaluator**\n",
    "3. Select the template `function_calling`  you just created\n",
    "4. Select Dataset as the **Target object** and make sure the Evaluator runs on **New dataset items**\n",
    "5. Configure the **Variable mapping** for your evaluator - this ensures that the correct parts of your traces get evaluated.\n",
    "    - `{{input}}` can be mapped to **Dataset item → Input**\n",
    "    - `{{expected_output}}` can be mapped to **Dataset item → Expected output**\n",
    "    - `{{output}}` can be mapped to **Trace → Output**\n",
    "6. Click **Save**\n",
    "\n",
    "![alt text](https://atla-ai.notion.site/image/attachment%3Af49c428d-d8ef-44ed-9a87-3d6936fe1252%3Aimage.png?table=block&id=1ba309d1-7745-80af-b04c-c558c7f8225f&spaceId=f08e6e70-73af-4363-9621-90e906b92ebc&width=2000&userId=&cache=v2)\n",
    "\n",
    "> You can set a sampling rate such that a % of the dataset is evaluated. For the purposes of this demonstration, we want to evaluate every trace so keep the sampling rate at 100%!\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aSSEEPlmW9bz"
   },
   "source": [
    "## Set Langfuse + OpenAI API keys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "executionInfo": {
     "elapsed": 15,
     "status": "ok",
     "timestamp": 1742816030988,
     "user": {
      "displayName": "Kyle Dai",
      "userId": "04739623040311656672"
     },
     "user_tz": 0
    },
    "id": "D9gBPNJjC8Qm"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.environ[\"LANGFUSE_PUBLIC_KEY\"] = \"\" # Your public key\n",
    "os.environ[\"LANGFUSE_SECRET_KEY\"] = \"\" # Your secret key\n",
    "\n",
    "os.environ[\"OPENAI_API_KEY\"] = \"\" # Your OpenAI API key"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zNUUhfMB1AXI"
   },
   "source": [
    "## Setup experiment runs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ApIVYqfWXFmi"
   },
   "source": [
    "### Install packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "executionInfo": {
     "elapsed": 8839,
     "status": "ok",
     "timestamp": 1742816039831,
     "user": {
      "displayName": "Kyle Dai",
      "userId": "04739623040311656672"
     },
     "user_tz": 0
    },
    "id": "E1ISYlFCXEs4"
   },
   "outputs": [],
   "source": [
    "pip install -qU datasets langfuse"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nkDPRb7G1chT"
   },
   "source": [
    "### Upload dataset to Langfuse"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DsWp-3OzQf3u"
   },
   "source": [
    "We processes data from the [Salesforce ShareGPT dataset](https://huggingface.co/datasets/arcee-ai/agent-data/viewer/default/train?f%5Bdataset%5D%5Bvalue%5D=%27glaive-function-calling-v2-extended%27&sql=SELECT+*%0AFROM+train%0AWHERE+dataset+%3D+%27salesforce_sharegpt%27%0ALIMIT+10%3B&views%5B%5D=train), extracting system prompts (that define the list of functions available), user queries, and the expected response.\n",
    "\n",
    "This data includes:\n",
    "\n",
    "1. `system_prompt` - Defines the AI's role as a function calling model, provides available functions in a <tools> XML format, and specifies the expected response format using <tool_call> tags.\n",
    "<br>\n",
    "\n",
    "2. `input` - A human query that asks the model to perform some task using the available functions.\n",
    "<br>\n",
    "\n",
    "3. `expected_output` - The expected model's response that includes one or more function calls in the specified XML format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 20734,
     "status": "ok",
     "timestamp": 1742816060575,
     "user": {
      "displayName": "Kyle Dai",
      "userId": "04739623040311656672"
     },
     "user_tz": 0
    },
    "id": "pbAqPxAG-EJ8",
    "outputId": "a39c2250-0dd7-4153-d10d-24bf5dccc5d6"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6cfc6c5fe23749a4990ee2f91eb6a9d3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split:   0%|          | 0/485874 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "eec7a4e88eeb4ac1a45b0696ce782caa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Filter:   0%|          | 0/485874 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of items after filtering: 10\n",
      "\n",
      "First item structure:\n",
      "{'conversations': [{'from': 'system', 'value': 'You are a function calling AI model. You may call one or more functions to assist with the user query. Don\\'t make assumptions about what values to plug into function. The user may use the terms function calling or tool use interchangeably.\\n\\nHere are the available functions:\\n<tools>[{\"name\": \"live_giveaways_by_type\", \"description\": \"Retrieve live giveaways from the GamerPower API based on the specified type.\", \"parameters\": {\"type\": {\"description\": \"The type of giveaways to retrieve (e.g., game, loot, beta).\", \"type\": \"str\", \"default\": \"game\"}}}]</tools>\\n\\nFor each function call return a json object with function name and arguments within <tool_call></tool_call> XML tags in the format:\\n<tool_call>{\"tool_name\": <function-name>, \"tool_arguments\": <args-dict>}</tool_call>'}, {'from': 'human', 'value': 'Where can I find live giveaways for beta access and games?'}, {'from': 'gpt', 'value': \"<tool_call>{'tool_name': 'live_giveaways_by_type', 'tool_arguments': {'type': 'beta'}}</tool_call>\\n<tool_call>{'tool_name': 'live_giveaways_by_type', 'tool_arguments': {'type': 'game'}}</tool_call>\"}], 'dataset': 'salesforce_sharegpt'}\n",
      "\n",
      "Processed 10 items\n"
     ]
    }
   ],
   "source": [
    "os.environ[\"LANGFUSE_HOST\"] = \"https://cloud.langfuse.com\" # EU region; LANGFUSE_HOST=\"https://us.cloud.langfuse.com\"\n",
    "\n",
    "from datasets import load_dataset\n",
    "from langfuse import Langfuse\n",
    "\n",
    "langfuse = Langfuse()\n",
    "\n",
    "# Load and process data\n",
    "dataset = load_dataset(\"arcee-ai/agent-data\")\n",
    "salesforce_data = list(dataset['train'].filter(lambda x: x['dataset'] == \"salesforce_sharegpt\"))[:10]  # convert to list first\n",
    "\n",
    "print(\"Number of items after filtering:\", len(salesforce_data))\n",
    "print(\"\\nFirst item structure:\")\n",
    "print(salesforce_data[0])\n",
    "\n",
    "# Process conversations into the format we want\n",
    "processed_items = []\n",
    "\n",
    "for item in salesforce_data:\n",
    "    try:\n",
    "        conversations = item['conversations']\n",
    "\n",
    "        system_prompt = next((conv['value'] for conv in conversations if conv['from'] == 'system'), '')\n",
    "        human_input = next((conv['value'] for conv in conversations if conv['from'] == 'human'), '')\n",
    "        model_output = next((conv['value'] for conv in conversations if conv['from'] == 'gpt'), '')\n",
    "\n",
    "        if system_prompt and human_input and model_output:\n",
    "            processed_items.append({\n",
    "                \"input\": {\n",
    "                    \"system_prompt\": system_prompt,\n",
    "                    \"query\": human_input\n",
    "                },\n",
    "                \"expected_output\": model_output\n",
    "            })\n",
    "    except Exception as e:\n",
    "        print(f\"Skipping malformed entry: {e}\")\n",
    "        print(f\"Item that caused error: {item}\")\n",
    "\n",
    "print(f\"\\nProcessed {len(processed_items)} items\")\n",
    "\n",
    "# Create the dataset\n",
    "langfuse.create_dataset(\n",
    "    name=\"salesforce_sharegpt\",\n",
    "    description=\"Function calling dataset from ArceeAI\"\n",
    ")\n",
    "\n",
    "# Upload items\n",
    "for item in processed_items:\n",
    "    langfuse.create_dataset_item(\n",
    "        dataset_name=\"salesforce_sharegpt\",\n",
    "        input=item[\"input\"],\n",
    "        expected_output=item[\"expected_output\"]\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xT06q54G1xrF"
   },
   "source": [
    "### Define experiment runs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "r_hCQn_mzQLA"
   },
   "source": [
    "We run the system prompt and user query through each model, and evaluate if the responses follow the proper tool call syntax (enclosed in <tool_call> tags) as defined in the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "executionInfo": {
     "elapsed": 2,
     "status": "ok",
     "timestamp": 1742816181760,
     "user": {
      "displayName": "Kyle Dai",
      "userId": "04739623040311656672"
     },
     "user_tz": 0
    },
    "id": "uk0Q7iZRUVY6"
   },
   "outputs": [],
   "source": [
    "from langfuse.openai import openai\n",
    "from langfuse.decorators import observe, langfuse_context\n",
    "import time\n",
    "\n",
    "@observe\n",
    "def run_function_calling(input, model_name):\n",
    "    # Models that support system messages\n",
    "    system_supported_models = [\"gpt-4\", \"gpt-4o\", \"gpt-3.5-turbo\"]\n",
    "\n",
    "    if model_name in system_supported_models:\n",
    "        messages = [\n",
    "            {\"role\": \"system\", \"content\": input[\"system_prompt\"]},\n",
    "            {\"role\": \"user\", \"content\": input[\"query\"]}\n",
    "        ]\n",
    "    else:\n",
    "        # For models that don't support system messages (the o-series), combine them in the user prompt\n",
    "        combined_prompt = f\"{input['system_prompt']}\\n\\nUser query: {input['query']}\"\n",
    "        messages = [\n",
    "            {\"role\": \"user\", \"content\": combined_prompt}\n",
    "        ]\n",
    "\n",
    "    completion = openai.chat.completions.create(\n",
    "        model=model_name,\n",
    "        messages=messages\n",
    "    ).choices[0].message.content\n",
    "\n",
    "    return completion\n",
    "\n",
    "def run_experiment(experiment_name, model_name):\n",
    "    dataset = langfuse.get_dataset(\"salesforce_sharegpt\")\n",
    "\n",
    "    for idx, item in enumerate(dataset.items):\n",
    "        with item.observe(run_name=experiment_name) as trace_id:\n",
    "            # Run application with the specific model\n",
    "            output = run_function_calling(item.input, model_name)\n",
    "\n",
    "        # Add a delay between requests to avoid rate limits\n",
    "        if idx < len(dataset.items) - 1:\n",
    "            time.sleep(2)  # 2 second delay"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Bk8EEql-oSOB"
   },
   "source": [
    "## Run experiments over different models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "executionInfo": {
     "elapsed": 155798,
     "status": "ok",
     "timestamp": 1742816348597,
     "user": {
      "displayName": "Kyle Dai",
      "userId": "04739623040311656672"
     },
     "user_tz": 0
    },
    "id": "TwdmeVbNoVFq"
   },
   "outputs": [],
   "source": [
    "# Run experiments with different models\n",
    "models = [\"o1-mini\", \"o3-mini\", \"gpt-4o\"]\n",
    "for model in models:\n",
    "    run_experiment(f\"function_calling_{model}\", model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yjNYNUUdEWDq"
   },
   "source": [
    "## Analyze results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "A4UfNT8DO5yb"
   },
   "source": [
    "<br>\n",
    "\n",
    "After the experiments with the different models finish running, head to **Datasets** in your sidebar and click on `salesforce_sharegpt` .\n",
    "\n",
    "<br>\n",
    "\n",
    "####**High-level model comparison**\n",
    "\n",
    "- You will see **Latency (s)** and **Average Total Cost ($)** graphs by default.\n",
    "- Add **Charts → Scores → Function_calling (Eval)** to see the average Selene scores for `function_calling`.\n",
    "\n",
    "  ![alt text](https://atla-ai.notion.site/image/attachment%3Aecd25ea4-86ed-4534-a647-5b39391e0b35%3Aimage.png?table=block&id=1bd309d1-7745-8012-b2eb-f4b07f9d2a5b&spaceId=f08e6e70-73af-4363-9621-90e906b92ebc&width=2000&userId=&cache=v2)\n",
    "\n",
    "Based on our limited test set of 10 samples, it seems the reasoning models are more suited to our agentic task!\n",
    "\n",
    "####**Fine-grained model comparison**\n",
    "\n",
    "- Select the three experiment runs and click **Actions (3 selected) → Compare**\n",
    "\n",
    "  ![alt text](https://atla-ai.notion.site/image/attachment%3Aecd25ea4-86ed-4534-a647-5b39391e0b35%3Aimage.png?table=block&id=1bd309d1-7745-8012-b2eb-f4b07f9d2a5b&spaceId=f08e6e70-73af-4363-9621-90e906b92ebc&width=2000&userId=&cache=v2)\n",
    "\n",
    "\n",
    "- In the comparison screen, we can inspect cases where certain models fail vs. others—highlighted by a `# Function_calling (Eval)` score of 0 in the cell.\n",
    "\n",
    "  ![alt text](https://atla-ai.notion.site/image/attachment%3Aeb428c05-cac0-46de-ba69-bcfc389ae1a6%3Aimage.png?table=block&id=1c0309d1-7745-8058-b75b-fa6f963f2979&spaceId=f08e6e70-73af-4363-9621-90e906b92ebc&width=2000&userId=&cache=v2)\n",
    "\n",
    "####**Next steps**\n",
    "\n",
    "Now that you've analyzed the models' performance, consider setting up additional Evaluators to measure other metrics that matter to your specific use case. If you have a collection of test user queries, upload this dataset and run comparative tests across different models."
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "jupyter_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
