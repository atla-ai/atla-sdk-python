{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_spCXMT3r128"
   },
   "source": [
    "# Monitoring your RAG app with Atla x Langfuse"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "31K7AlAQrA0f"
   },
   "source": [
    "This notebook demonstrates how to monitor a Retrieval-Augmented Generation (RAG) application using **Atla Selene for evaluation** and **Langfuse for observability**. If you'd like a visual walkthrough, check out our [demo video](https://www.youtube.com/watch?v=4TciraGerv8).\n",
    "\n",
    "We use Alphabet's Q4 2024 earnings call transcript as an example document.\n",
    "\n",
    "We build a Gradio application with a complete RAG pipeline that you can play around with. Traces will automatically be sent to Langfuse and scored by Selene. \n",
    "\n",
    "<br>\n",
    "\n",
    "**Requirements:**\n",
    "\n",
    "- An Atla account - you can sign up for free [here](https://www.atla-ai.com/sign-up)\n",
    "- A Langfuse account - you can sign up for free [here](https://cloud.langfuse.com/auth/sign-up)\n",
    "- An OpenAI API key - you can sign up for free [here](https://platform.openai.com/signup)\n",
    "\n",
    "**Get started**\n",
    "\n",
    "1. Follow the steps below in **Setup Atla on Langfuse**.\n",
    "2. Set your Langfuse + OpenAI API keys.\n",
    "3. Run the rest of the functions to load the RAG app.\n",
    "4. Happy chatting! You'll get live quality assessments from Selene in your Langfuse traces, allowing you to accurately monitor your application's performance over time.\n",
    "\n",
    "\n",
    "> Try prompting the chatbot adversarially to see if it hallucinates. Selene will detect poor responses and assign lower scores - helping you identify areas for improvement.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fSh3ZbhjlqAA"
   },
   "source": [
    "## Setup Atla on Langfuse"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YcmX7FpwYmU2"
   },
   "source": [
    "Navigate to your project on [cloud.langfuse.com](cloud.langfuse.com):\n",
    "\n",
    "<br>\n",
    "\n",
    "**Add your Atla API key to your Langfuse project:**\n",
    "\n",
    "1. Head to **Settings** → **LLM Connections** and select **+** **Add new LLM API key.**\n",
    "2. Set `atla` as the **Provider name** and select `atla` from the **LLM adapter** dropdown .\n",
    "3. The API Base URL will automatically be filled in. Paste your Atla API key beginning with “pk-…” into the **API Key** field.\n",
    "4. Leave **Enable default models** on.\n",
    "5. Click **Save new LLM API key.**\n",
    "\n",
    "![alt text](https://atla-ai.notion.site/image/attachment%3Aed7c7d92-b0bf-464f-9f43-83df6de65a5e%3Aimage.png?table=block&id=1bc309d1-7745-80fd-9f2d-cdd40f3005cf&spaceId=f08e6e70-73af-4363-9621-90e906b92ebc&width=2000&userId=&cache=v2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2AP2Iuu1lgEp"
   },
   "source": [
    "**Add an LLM-as-a-Judge template**\n",
    "\n",
    "1. Head to **Evaluation → LLM-as-a-Judge** in your sidebar and select **Templates.**\n",
    "2. Click **+ New Template.**\n",
    "3. Select `atla` as the **Model Provider** and select `atla-selene` as the **Model name.**\n",
    "4. Let’s evaluate the retrieval component of our RAG app— select `Contextrelevance` from the default eval templates dropdown.\n",
    "5. Adjust the prompts under **Reasoning** and **Score**— Selene will calibrate its score and feedback based on your specifications.\n",
    "6. Click **Save.**\n",
    "\n",
    "![alt text](https://atla-ai.notion.site/image/attachment%3A8cbdd3b7-4e9d-4548-981d-14c8f1fbe141%3Aimage.png?table=block&id=1bc309d1-7745-8031-aab8-d33bfac00586&spaceId=f08e6e70-73af-4363-9621-90e906b92ebc&width=2000&userId=&cache=v2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OOIZBSbFlHV-"
   },
   "source": [
    "**Add a new Evaluator configuration:**\n",
    "\n",
    "1. Head to **Evaluation → LLM-as-a-Judge** in your sidebar and select **Evaluators**.\n",
    "2. Click **+ New evaluator**.\n",
    "3. Select the template `Contextrelevance` you just created.\n",
    "4. Configure the **Variable mapping** for your evaluator to ensure the correct components of your traces are evaluated:\n",
    "    - `{{query}}` can be mapped to **Trace → Input**\n",
    "    - `{{context}}` can be mapped to **Span → Retrieval → Output**\n",
    "5. Click **Save.**\n",
    "\n",
    "![alt text](https://atla-ai.notion.site/image/attachment%3A9491785d-ae38-44ad-915e-7e1fc84792fc%3Aimage.png?table=block&id=1b9309d1-7745-80db-bd16-ca4ef455c723&spaceId=f08e6e70-73af-4363-9621-90e906b92ebc&width=2000&userId=&cache=v2)\n",
    "\n",
    "> You can configure your evaluator such that it runs only when a target filter is passed, or you can set a sampling rate such that a % of traces are evaluated. For the purposes of this demonstration, we want to evaluate every trace so keep the sampling rate at 100%!\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EITUVAotnIZv"
   },
   "source": [
    "## Set Langfuse + OpenAI API keys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zlns9c8Dr88q"
   },
   "outputs": [],
   "source": [
    "website_url = \"https://abc.xyz/2024-q4-earnings-call/\" # The website containing the document to be queried - we have set the Q4 24 earnings transcript for Alphabet\n",
    "\n",
    "import os\n",
    "\n",
    "os.environ[\"LANGFUSE_PUBLIC_KEY\"] = \"\" # Your public LF key\n",
    "os.environ[\"LANGFUSE_SECRET_KEY\"] = \"\" # Your secret LF key\n",
    "os.environ[\"OPENAI_API_KEY\"] = \"\" # Your OpenAI key"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sUNt8l-2XHx4"
   },
   "source": [
    "## Setup RAG pipline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ApIVYqfWXFmi"
   },
   "source": [
    "### Install packages for notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 44923,
     "status": "ok",
     "timestamp": 1742394370614,
     "user": {
      "displayName": "Jackson Golden",
      "userId": "09124791872741753720"
     },
     "user_tz": 0
    },
    "id": "E1ISYlFCXEs4",
    "outputId": "0f637188-2f5f-4eff-eedc-19277ad99453"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/981.5 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m981.5/981.5 kB\u001b[0m \u001b[31m30.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m61.0/61.0 kB\u001b[0m \u001b[31m4.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m149.4/149.4 kB\u001b[0m \u001b[31m9.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.5/2.5 MB\u001b[0m \u001b[31m70.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m134.8/134.8 kB\u001b[0m \u001b[31m10.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m60.9/60.9 kB\u001b[0m \u001b[31m4.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m264.3/264.3 kB\u001b[0m \u001b[31m19.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m46.2/46.2 MB\u001b[0m \u001b[31m12.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m322.2/322.2 kB\u001b[0m \u001b[31m19.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m9.5/9.5 MB\u001b[0m \u001b[31m75.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.8/1.8 MB\u001b[0m \u001b[31m54.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m100.9/100.9 kB\u001b[0m \u001b[31m7.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m94.9/94.9 kB\u001b[0m \u001b[31m7.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.0/1.0 MB\u001b[0m \u001b[31m41.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m41.2/41.2 kB\u001b[0m \u001b[31m3.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m46.0/46.0 kB\u001b[0m \u001b[31m3.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m18.3/18.3 MB\u001b[0m \u001b[31m51.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m571.1/571.1 kB\u001b[0m \u001b[31m27.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m11.3/11.3 MB\u001b[0m \u001b[31m67.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m72.0/72.0 kB\u001b[0m \u001b[31m5.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m42.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m492.9/492.9 kB\u001b[0m \u001b[31m24.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.3/62.3 kB\u001b[0m \u001b[31m4.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m590.6/590.6 kB\u001b[0m \u001b[31m27.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m167.6/167.6 kB\u001b[0m \u001b[31m11.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.1/3.1 MB\u001b[0m \u001b[31m54.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m59.9/59.9 kB\u001b[0m \u001b[31m4.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m434.9/434.9 kB\u001b[0m \u001b[31m27.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.1/2.1 MB\u001b[0m \u001b[31m66.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.9/50.9 kB\u001b[0m \u001b[31m3.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m302.3/302.3 kB\u001b[0m \u001b[31m23.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m114.6/114.6 kB\u001b[0m \u001b[31m8.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h  Building wheel for langdetect (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "jupyter-server 1.24.0 requires anyio<4,>=3.1.0, but you have anyio 4.9.0 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "pip install -qU langchain-text-splitters langchain-community langgraph langchain-openai langfuse gradio selenium unstructured numpy==1.26.4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aSSEEPlmW9bz"
   },
   "source": [
    "### Initialize models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "n4wR4FkaSN2m"
   },
   "outputs": [],
   "source": [
    "from langchain.chat_models import init_chat_model\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "\n",
    "llm = init_chat_model(\"gpt-4o-mini\", model_provider=\"openai\") # We choose our chat model\n",
    "embeddings = OpenAIEmbeddings(model=\"text-embedding-3-large\") # We choose our embeddings model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DLiYlP_xtlha"
   },
   "source": [
    "### Define functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bgwzHcitrnZ0"
   },
   "source": [
    "We set up a complete RAG pipeline with:\n",
    "\n",
    "- Document loading\n",
    "- Text chunking and metadata tagging\n",
    "- Vector embedding and storage\n",
    "- Query analysis and structuring\n",
    "- Retrieval based on relevance\n",
    "- Response generation based on context\n",
    "\n",
    "The implementation uses Langfuse's observation decorators to track each step of the pipeline, capturing inputs, outputs, and metadata."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 391
    },
    "executionInfo": {
     "elapsed": 2363,
     "status": "error",
     "timestamp": 1742394449069,
     "user": {
      "displayName": "Jackson Golden",
      "userId": "09124791872741753720"
     },
     "user_tz": 0
    },
    "id": "Otpz1BaltnAq",
    "outputId": "40ad0e5e-bbf9-4920-d9be-692c6f3237dc"
   },
   "outputs": [],
   "source": [
    "os.environ[\"LANGFUSE_HOST\"] = \"https://cloud.langfuse.com\" # EU region\n",
    "os.environ[\"LANGCHAIN_TRACING_V2\"] = \"false\" # Prevents error when using OSS Langchain\n",
    "os.environ[\"USER_AGENT\"] = \"myagent\" # Prevents error when using OSS Langchain\n",
    "\n",
    "from typing import Literal\n",
    "from langfuse.decorators import observe, langfuse_context\n",
    "from langfuse import Langfuse\n",
    "import bs4\n",
    "from langchain import hub\n",
    "from langchain_community.document_loaders import WebBaseLoader\n",
    "from langchain_core.documents import Document\n",
    "from langchain_core.vectorstores import InMemoryVectorStore\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langgraph.graph import START, StateGraph\n",
    "from typing_extensions import Annotated, List, TypedDict\n",
    "from langchain_community.document_loaders import SeleniumURLLoader\n",
    "\n",
    "# Load and chunk contents of the website\n",
    "loader = SeleniumURLLoader(urls=[website_url])\n",
    "docs = loader.load()\n",
    "\n",
    "# Chunking function for document\n",
    "def manual_split(document, chunk_size=800, overlap=200):\n",
    "    text = document.page_content\n",
    "    splits = []\n",
    "\n",
    "    # Simple character-based chunking\n",
    "    for i in range(0, len(text), chunk_size - overlap):\n",
    "        chunk = text[i:i + chunk_size]\n",
    "        if chunk:  # Ensure we're not adding empty chunks\n",
    "            doc = Document(page_content=chunk, metadata=document.metadata.copy())\n",
    "            splits.append(doc)\n",
    "\n",
    "    return splits\n",
    "\n",
    "# Apply splitting\n",
    "all_splits = []\n",
    "for doc in docs:\n",
    "    all_splits.extend(manual_split(doc))\n",
    "\n",
    "# Update metadata\n",
    "total_documents = len(all_splits)\n",
    "third = total_documents // 3\n",
    "\n",
    "for i, document in enumerate(all_splits):\n",
    "    if i < third:\n",
    "        document.metadata[\"section\"] = \"beginning\"\n",
    "    elif i < 2 * third:\n",
    "        document.metadata[\"section\"] = \"middle\"\n",
    "    else:\n",
    "        document.metadata[\"section\"] = \"end\"\n",
    "\n",
    "\n",
    "# Index chunks\n",
    "vector_store = InMemoryVectorStore(embeddings)\n",
    "_ = vector_store.add_documents(all_splits)\n",
    "\n",
    "\n",
    "# Define schema for search\n",
    "class Search(TypedDict):\n",
    "    \"\"\"Search query.\"\"\"\n",
    "\n",
    "    query: Annotated[str, ..., \"Search query to run.\"]\n",
    "    section: Annotated[\n",
    "        Literal[\"beginning\", \"middle\", \"end\"],\n",
    "        ...,\n",
    "        \"Section to query.\",\n",
    "    ]\n",
    "\n",
    "# Define prompt for question-answering\n",
    "prompt = hub.pull(\"rlm/rag-prompt\")\n",
    "\n",
    "\n",
    "# Define state for application\n",
    "class State(TypedDict):\n",
    "    question: str\n",
    "    query: Search\n",
    "    context: List[Document]\n",
    "    answer: str\n",
    "\n",
    "# Structure user query better\n",
    "@observe()\n",
    "def analyze_query(state: State):\n",
    "    with_structured = llm.with_structured_output(Search)\n",
    "    query = with_structured.invoke(state[\"question\"])\n",
    "    return {\"query\": query}\n",
    "\n",
    "\n",
    "# Retrieve relevant documents based on structured user query\n",
    "@observe(as_type=\"retrieval\")\n",
    "def retrieve(state: State):\n",
    "    query = state[\"query\"]\n",
    "\n",
    "    # Try without any specific filter first\n",
    "    retrieved_docs = vector_store.similarity_search(\n",
    "        query[\"query\"],\n",
    "        k=4  # Set explicit number of results\n",
    "    )\n",
    "\n",
    "    # This approach looks for partial matches in the section value\n",
    "    if len(retrieved_docs) == 0 and query.get(\"section\"):\n",
    "        retrieved_docs = vector_store.similarity_search(\n",
    "            query[\"query\"],\n",
    "            filter=lambda doc: query[\"section\"] in str(doc.metadata.get(\"section\", \"\")),\n",
    "            k=4\n",
    "        )\n",
    "\n",
    "    docs_content = \"\\n\\n\".join(doc.page_content for doc in retrieved_docs)\n",
    "\n",
    "    langfuse_context.update_current_observation(\n",
    "        input={\"query\": query[\"query\"], \"section\": query.get(\"section\", \"any\")},\n",
    "        output={\n",
    "            \"num_docs\": len(retrieved_docs),\n",
    "            \"retrieved_content\": docs_content  # Add full context\n",
    "        }\n",
    "    )\n",
    "    return {\"context\": retrieved_docs}\n",
    "\n",
    "# Generate response based on user query and context retrieved\n",
    "@observe(as_type=\"generation\")\n",
    "def generate(state: State):\n",
    "\n",
    "    docs_content = \"\\n\\n\".join(doc.page_content for doc in state[\"context\"])\n",
    "\n",
    "    messages = prompt.invoke({\"question\": state[\"question\"], \"context\": docs_content})\n",
    "    response = llm.invoke(messages)\n",
    "    return {\"answer\": response.content}\n",
    "\n",
    "# Create a wrapper function that will be the main trace sent to Langfuse\n",
    "@observe()\n",
    "def process_rag_pipeline(question: str, trace_id: str = None, session_id: str = None):\n",
    "    \"\"\"Main function that creates the top-level trace\"\"\"\n",
    "    # Pass trace_id if you want to use a custom one\n",
    "    kwargs = {\"langfuse_observation_id\": trace_id} if trace_id else {}\n",
    "\n",
    "    # Initialize state\n",
    "    state = {\"question\": question}\n",
    "\n",
    "    # Update trace with metadata\n",
    "    langfuse_context.update_current_trace(\n",
    "        name=\"RAG Pipeline\",\n",
    "        user_id=\"demo_user\", # Optional\n",
    "        session_id=session_id, # Optional\n",
    "        tags=[\"rag\", \"demo\"]  # Optional\n",
    "    )\n",
    "\n",
    "    # Execute pipeline steps within the same trace context\n",
    "    state.update(analyze_query(state, **kwargs))\n",
    "    state.update(retrieve(state, **kwargs))\n",
    "    state.update(generate(state, **kwargs))\n",
    "\n",
    "    return state[\"answer\"]\n",
    "\n",
    "# Create a function that processes user input for app\n",
    "def process_query(user_input):\n",
    "    result = process_rag_pipeline(user_input, session_id=session_id)\n",
    "    if isinstance(result, dict):\n",
    "        return result[\"answer\"]\n",
    "    return str(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kc25TzQU46GD"
   },
   "source": [
    "## Build RAG app"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 625
    },
    "executionInfo": {
     "elapsed": 1769,
     "status": "ok",
     "timestamp": 1742244454731,
     "user": {
      "displayName": "Kyle Dai",
      "userId": "04739623040311656672"
     },
     "user_tz": 0
    },
    "id": "zUhEWLUs4CAV",
    "outputId": "3d7104f6-169b-46ac-d4f7-0534c2343901"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running Gradio in a Colab notebook requires sharing enabled. Automatically setting `share=True` (you can turn this off by setting `share=False` in `launch()` explicitly).\n",
      "\n",
      "Colab notebook detected. To show errors in colab notebook, set debug=True in launch()\n",
      "* Running on public URL: https://af96cc7617c984f82f.gradio.live\n",
      "\n",
      "This share link expires in 72 hours. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"https://af96cc7617c984f82f.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning, module=\"langsmith.client\")\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning, message=\".*type.*parameter.*\")\n",
    "\n",
    "import gradio as gr\n",
    "import uuid\n",
    "\n",
    "session_id = str(uuid.uuid4())\n",
    "\n",
    "# Create a simple Gradio interface\n",
    "with gr.Blocks() as demo:\n",
    "    gr.Markdown(\"# RAG Chat Interface\")\n",
    "\n",
    "    with gr.Row():\n",
    "        with gr.Column():\n",
    "            chatbot = gr.Chatbot(height=400)\n",
    "            msg = gr.Textbox(label=\"🧑 User input\", placeholder=\"Ask a question about the document! You can try 'How were financial results?'\")\n",
    "\n",
    "    chat_history = []\n",
    "\n",
    "    def respond(message, chat_history):\n",
    "        bot_message = process_query(message)\n",
    "        chat_history.append((message, bot_message))\n",
    "        return \"\", chat_history\n",
    "\n",
    "    msg.submit(respond, [msg, chatbot], [msg, chatbot])\n",
    "\n",
    "demo.launch(debug=False, quiet=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "G_wI1Q8K15s9"
   },
   "source": [
    "Click on the public URL above to view the app in full screen. <br>\n",
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6zwymy-M18UL"
   },
   "source": [
    "## Monitor your performance with Selene"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XKZ5zaZG3HfA"
   },
   "source": [
    "1. After using the chatbot, head to **Tracing** → **Traces** in your sidebar and select a trace with the name ‘RAG pipeline’.\n",
    "\n",
    "2. Once there, you can analyze different components of your RAG pipeline:\n",
    "    \n",
    "    ![image.png](https://atla-ai.notion.site/image/attachment%3Addc6d1af-cbd9-4469-bc26-b598ba22ac22%3Aimage.png?table=block&id=1b9309d1-7745-803e-ba53-d7df90d8aa4d&spaceId=f08e6e70-73af-4363-9621-90e906b92ebc&width=2000&userId=&cache=v2)\n",
    "    \n",
    "3. From the highest-level Trace, click on Scores to view Selene's score and critique:\n",
    "    \n",
    "    ![image.png](https://atla-ai.notion.site/image/attachment%3Af8ec919b-68ba-4df9-bc43-279d1eef8191%3Aimage.png?table=block&id=1b9309d1-7745-809d-8105-df99108d734e&spaceId=f08e6e70-73af-4363-9621-90e906b92ebc&width=2000&userId=&cache=v2)\n",
    "    \n",
    "  - There might be a 10-second delay before the evaluation score appears, due to the Delay setting in the Evaluator.\n",
    "\n",
    "> By regularly monitoring Selene's scores over time, you can detect model drift, privacy vulnerabilities, outdated vector databases, retrieval issues, and other potential problems!\n",
    "\n",
    "#### **Next steps**\n",
    "\n",
    "Now that you've set up one monitor, try setting up additional Evaluators to measure other metrics that matter to your specific use case!\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [
    "fSh3ZbhjlqAA",
    "sUNt8l-2XHx4",
    "ApIVYqfWXFmi",
    "aSSEEPlmW9bz",
    "DLiYlP_xtlha",
    "kc25TzQU46GD"
   ],
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
